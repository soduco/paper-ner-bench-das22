% !TeX root = ../main-paper.tex
\section{Conclusion and future works}
We assessed the performance of three modern OCR systems on a set of historical sources of great interest in social history.
Although \peroocr clearly outperforms its competitors, the qualitative analysis of OCR errors shows that its failure cases are not the same as Tesseract.
This calls for leveraging both OCR systems in a complementary way to get the best of the two worlds.
%Moreover, we evaluate two deep-learning-based NER approaches in terms of their sensibility to the amount of training data and to noisy OCR texts.
The evaluation of SpaCy NER and CamemBERT (with and without pre-training) showed that BERT-based NER can benefit from pre-training and fine-tuning on a corpus produced with the same process than the texts to annotate.
%However, the results are not as good as for clean text: it is therefore interesting to investigate how to improve the OCR results or to correct the text in a postprocessing step.
Furthermore, it seems that all three models achieve good performance with relatively few training examples.
With a F1-score of 92\% with only 49 training examples, the pre-trained CamemBERT model is a good choice to serve as a bootstrapping model to quickly produce large training sets and therefore lower the burden of creating a ground truth from scratch.
Besides, as directory entries always have the same structure - at least within a given index - we could take advantage of NER results and some simple rules to identify entries within pages instead of relying on the page layout only, or even interactively generate per-index NER models to take advantage of the low amount of training samples required.
%Interesting points to discuss:
%\begin{itemize}
%    \item can we train on noisy data? (without manual OCR correction?) => future work? cf Pero OCR training procedure?
%    \item do we need better OCR systems or better post-correction techniques (if NER is reliable enough)?
%    \item Construction of the lexicon and associated cost \nathalie{là je ne comprends pas}
%\end{itemize}
%Ideas for future work:
%\begin{itemize}
%    \item Evaluate this: fine-tune Spacy NER on a few samples to generated a bootstrap dataset, feed it to BERT (cleaned or raw) to create an good model with minimal efforts ? \nathalie{les résultats de la table 2 ne plaident pas vraiment pour ça: même sur les très petits corpus, spacy est tours derrière...}
%    \item Use ML to infer regexes ?
%    \item As directories entries are always structured the same way (in a given list at least): use NER to identify entries within pages to save the burden of having to rely on the page layout ?
%\end{itemize}