% !TeX root = ../main-paper.tex
\section{Results}

\subsection{OCR evaluation}

\begin{figure}

\subcaptionbox{}[.5\linewidth]{
\begin{tabular}{rll}
          & CER & CER$_{norm.}$ \\
\midrule
Pero      & 3.78\% & 3.76\% \\   
Tesseract & 6.56\% & 6.45\% \\
\bottomrule
\end{tabular}

\bigskip

\includegraphics[width=\linewidth]{images/ocr-eval-2.pdf}
}
\subcaptionbox{}[.5\linewidth]{
\includegraphics[width=\linewidth]{images/ocr-eval-1.pdf}
}
\caption{Character error rates at entry-level for Pero OCR and Tesseract. (a) Global CER and distribution of the CER per entry. (b):
joint plot of the dataset showing that the two systems do not fail on the same entries.}  
\end{figure}

\subsection{Experiment 1: NER sensibility to the number of training samples}

Qualitative results
\textbf{TODO random samples of results + selection of failure cases}


Quantitative results: table + graph ideally (with training set size info)

\begin{table}[h!]
\centering
\caption{Experimental results of the NER models performances when trained on varying numbers of examples}
\begin{tabular}{llrrrrrrrr}
       & Trainset Size &  49   &  99   &  199  &  398  &  796  &  1593 &  3186 &  6373 \\
       & \% & 0.8   & 1.6   & 3.1   & 6.2   & 12.5  & 25.0  & 50.0  & 100.0 \\
\midrule\bottomrule
\multirow{3}{*}{\rotatebox{90}{F1 score}} & Camembert &  89.5 &  90.5 &  92.7 &  93.3 &  94.1 &  94.9 &  94.6 &  95.1 \\
       & Camembert-pretrained &  92.4 &  92.9 &  93.7 &  93.8 &  93.8 &  94.0 &  94.6 &  94.4 \\
       & SpaCy NER &  87.0 &  89.0 &  90.3 &  91.9 &  92.1 &  92.8 &  93.2 &  93.5 \\
\cline{1-10}
\multirow{3}{*}{\rotatebox{90}{Precision}} & Camembert &  87.4 &  88.7 &  91.5 &  92.7 &  93.3 &  94.9 &  93.9 &  95.1 \\
       & Camembert-pretrained &  91.2 &  91.9 &  92.9 &  93.0 &  93.0 &  93.2 &  94.2 &  93.9 \\
       & SpaCy NER &  85.6 &  87.7 &  90.0 &  92.0 &  92.4 &  92.8 &  93.1 &  93.7 \\
\cline{1-10}
\multirow{3}{*}{\rotatebox{90}{Recall}} & Camembert &  91.6 &  92.5 &  93.9 &  93.9 &  94.9 &  94.9 &  95.4 &  95.1 \\
       & Camembert-pretrained &  93.7 &  94.0 &  94.4 &  94.7 &  94.6 &  94.8 &  95.0 &  94.9 \\
       & SpaCy NER &  88.6 &  90.4 &  90.7 &  91.7 &  91.9 &  92.8 &  93.3 &  93.4 \\
\end{tabular}
\label{tab:experiment-1-models-performances}
\end{table}


\begin{figure}[h!]
	   \center{\includegraphics[width=\textwidth]
	       {images/experiment-1-models-performances.pdf}}
	  \vspace{3in}
	  \caption{\label{fig:f1-vs-trainsize} Models F1 score on unseen data vs trainset size}
\end{figure}
	                                        


\subsection{Experiment 2: NER in the presence of noisy OCR texts}

Qualitative results
\textbf{TODO random samples of results + selection of failure cases}

Quantitative results: table + graph ideally (with OCR noise, same format as previous)

Table : NER NN models VS train on {noisy, clean} datasets VS evaluate on {noisy, clean} datasets

\begin{tabular}{lrrrrrr}
{} & \multicolumn{3}{c}{Camembert.pretrained} & \multicolumn{3}{c}{Camembert} \\
{} &                  ref & pero-ocr & tesseract &       ref & pero-ocr & tesseract \\
\midrule
Precision &                 96.5 &     93.3 &      90.3 &      96.1 &     92.4 &      89.7 \\
Recall    &                 96.5 &     92.6 &      91.3 &      96.6 &     92.5 &      91.2 \\
F1 score        &                 96.5 &     92.9 &      90.8 &      96.4 &     92.5 &      90.5 \\
\bottomrule
\end{tabular}


\begin{table}[h!]
\caption{Camembert vs noise}
\centering
\begin{tabular}{ll|cc|c}
 & & \multicolumn{2}{c|}{Training data} & \\
 & & noisy & clean &   \\ 
\cline{1-4}
\multirow{3}{*}{Test data}& noisy gold (Tesseract) & f1 & f1 & \\
                            & noisy gold (Pero-OCR) & f1 & f1 & \\ 
                            & reference gold & f1 & f1 & \\ 
                            & reference gold & f1 & f1 & \\
\cline{1-4}
\end{tabular}
\end{table}



Opt. use synthetic text perturbation as well? Maybe not interesting and too artificial if we have access to 2+ OCR systems.
(original OCR from BNF, Tesseract 4, Pero OCRâ€¦)


\subsection{Discussion}
Interesting points to discuss:
\begin{itemize}
    \item can we train on noisy data? (without manual OCR correction?) => future work? cf Pero OCR training procedure?
    \item do we need better OCR systems or better post-correction techniques (if NER is reliable enough)?
    \item Construction of the lexicon and associated cost
\end{itemize}
