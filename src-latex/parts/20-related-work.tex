% !TeX root = ../main-paper.tex
\section{OCR and NER on historical texts}
\label{sec:related-work}

The directory processing pipeline presented in \cite{bell2020automated} includes an OCR step, done with Tesseract, and a NER step to identify company names and addresses, performed using regular expressions.
This section reviews existing OCR and NER approaches on historical texts and presents some works assessing the effects of OCR quality on the NER performance and the proposed solutions. 

\subsection{Optical Character Recognition of historical texts}
Among the large number of OCR solutions, being either open, free, or paid software, available as libraries, Python packages, binaries, or cloud API, not all options seem suitable for historical document processing.
We chose to avoid in our study paid and closed-source solutions.
This notably discards Transkribus~\cite{transkribus}, which relies on the commercial system ABBYY's Finereader as well as on two handwritten transcription engines, to process text.
% for Transkribus HTR engines, there is one closed source and one apparently open source
% we should test the open source one: https://github.com/jpuigcerver/pylaia


Most of the current state-of-the-art open-source OCR systems, like Tesseract \cite{smith2007overview}, OCRopus \cite{breuel2008ocropus}, and the recent Kraken~\cite{kraken}, Calamari~\cite{wick_calamari_2020} and \peroocr \cite{kohut2021ts} are based on a pipeline of convolutional neural networks (CNNs) and long short-term memory networks (LSTM).
Although this model produces good results with modern texts, it still faces challenges with ancient texts, such as the lack of annotated data for learning, or different transcription styles for training data.
%
% \cite{martinek2019hybrid} propose an approach to generate synthetic annotated text for historical OCR training, based on manually collected characters from historical text images.
% This work proposes to train an OCR system based on a CNN-LSTM network with synthetic data and then to fine-tune the model with some pages of real historical annotated text.
% The results show that this approach gives state-of-the-art results. 

To overcome the limitations due to different transcription styles in training data, \peroocr adds a Transcription Style Block layer to a classical model based on a CNN and a Recurrent Neural Network components \cite{kohut2021ts}.
This block takes the image of the text and a Transcription Style Identifier as inputs and helps the network decide what kind of transcription style to use as output.

% In this study, we consider Tesseract version 4, Kraken and \peroocr.
% They were selected because of their ease of use (in the case of Tesseract and Kraken)
% or because of their performance (in the case of \peroocr),
% but further work should include more systems from our initial selection.

% Finally don't mention OCR-D as it relies on either Tesseract, Kraken, Ocropus or Calamari

% PERO refs
% O Kodym, M Hradiš: Page Layout Analysis System for Unconstrained Historic Documents. ICDAR, 2021.
% M Kišš, K Beneš, M Hradiš: AT-ST: Self-Training Adaptation Strategy for OCR in Domains with Limited Transcriptions. ICDAR, 2021.
% J Kohút, M Hradiš: TS-Net: OCR Trained to Switch Between Text Transcription Styles. ICDAR, 2021.


\subsection{Named Entity Recognition in historical texts}
\label{subsection:stoa-ner-on-historical-texts}

Many approaches have been designed to recognise named entities, ranging from handcrafted rules to supervised approaches \cite{nadeau2007}.
Rule based approaches look for portions of the text that match patterns like in \cite{bell2020automated,nouvel2011} or dictionary (gazetteers, author lists, etc.) entries like in \cite{mansouri2008,maurel2011}.
Such kind of approaches achieve very good results when applied to a specialised domain corpus and when an exhaustive lexicon are available, but at high system engineering cost \cite{nadeau2007}. 

Supervised approaches include both approaches implementing supervised learning algorithms with careful text feature engineering, and deep learning based approaches which automatically build their own features to classify tokens into named entity categories.
In recent years, the latter have grown dramatically, yielding state-of-the-art performances as shown in the recent survey proposed by \cite{li2020}. This survey concludes that fine-tuning general-purpose contextualised language models with domain-specific data is very likely to give good performance for use cases with domain-specific texts and few training data. This strategy has been adopted by \cite{Labusch2020NamedED} to extract named entities in OCRed historical texts in German, French, and English. However, the NER performance drops significantly as the quality of the OCR decreases and is correlated with its decrease.

Several recent studies have focused on the impact of OCR quality on NER results. Most of the time, they have evaluated NER approaches based on deep learning architectures as they seem to adapt more easily to OCR errors than rule-based or more classical supervised approaches.
\cite{van2020assessing} use the English model \textit{en-core-web-lg} provided by SpaCy \cite{spacy} library to perform NER on a corpus made of many journal articles with different levels of OCR errors.
For each OCRed article, a ground truth text is available so that the Word Error Rate (WER) can be computed.
The performance of the NER model with respect to OCR quality is eventually assessed by computing the F1-score for each NER class and each article, i.e., each WER value.
\cite{hamdi2020assessing} performed a similar but more extensive evaluation on four supervised NER models: CoreNLP using Conditional Random Fields and three deep neural models, BLSTM-CNN, BLSTM-CRF, and BLSTM-CNN-CRF.
They tested them on CoNLL-02 and CoNLL-03 NER benchmark corpora, degraded by applying four different types of OCR noise. Overall, NER F-measure drops from 90\% to 50\% when the Word Error Rate increases from 8\% to 50\%. However, models based on deep neural networks seem less sensitive to OCR errors. 
Two approaches have been proposed by \cite{huynh2020use} and \cite{marz2021data}  to reduce the negative impact of OCR errors on NER performance on historical texts.
The former applies a spelling correction tool to several corpora with variable OCR error rates. 
As long as OCR errors remain low ($CER<2\%$ and $WER<10\%$), this strategy makes it possible to maintain good NER results. However, the F1-score starts to decrease significantly when OCR errors exceed these thresholds.
The latter work focuses on adapting the training data to facilitate the generalisation of an off-the-shelf NER model from modern texts to historical texts.
Finally, reusing a model trained on clean modern data, including embeddings computed on a historical corpus, and fine-tuned on a noisy historical ground truth has proven to be the most effective strategy.

\subsection{Pipeline summary}
\label{sec:pipeline-summary}

Based on those works, we chose to test three OCR systems, namely, Tesseract, PERO OCR, and Kraken. We also adopt two deep-learning-based French language models, available in packaged software libraries, already trained for the NER task and that we can adapt to the domain of historical directories: SpaCy NLP pipelines and CamemBERT.
\Cref{fig.pipeline} depicts the evaluation protocol used to assess the combined performance of these OCR and NER systems. 

\textbf{Tesseract} is a long-living project, born as a closed-sourced OCR at Hewlett-Packard in the eighties, it was progressively modernized, then open-sourced in 2005. From 2006 until November 2018, it was developed by Google and is still very active. We used in our tests version 4.1.1, released Dec. 26, 2019. Version 5, released on Nov. 30, 2021, has not been integrated in our tests yet.

\textbf{Kraken} is a project created by Benjamin Kiessling several years ago (development can be traced back to 2015), and is actively used in the open-source eScriptorium project \cite{kiessling_escriptorium_2019}.
As no pre-trained model for modern French was easily available, we used the default English text recognition model trained on modern printed English by Benjamin Kiessling on 2019. Models can be easily found and downloaded thanks to their hosting on Zenodo.

\textbf{\peroocr} is a very recent project (started in 2020) from the Brno University of Technology in Czech Republic.
Their authors used many state-of-the-art techniques to train it very efficiently.
We used the version from the master branch of their GitHub repository, updated on Sep 15, 2021.
We used the pre-trained weights provided by the authors on the same repository, created on Oct. 9, 2020 from European texts with Latin, Greek, and Cyrillic scripts.
% https://github.com/DCGM/pero-ocr
% https://www.fit.vut.cz/~ihradis/pero/pero_eu_cz_print_newspapers_2020-10-09.tar.gz

\textbf{SpaCy} is a software library that offers NLP components assembled in modular pipelines specialised by language.
Although BERT is available in the latest version of SpaCy (v3), the pipeline for French does not provide a NER layer at the time of our experiments (as of January 2022).
Hence, we rely on SpaCy's ad hoc pipeline trained on French corpora and capable of Named Entity Recognition.
% deep-sequoia and wikiner-fr, capable of named entity recognition.\textit{fr\_core\_news\_lg}\footnote{https://spacy.io/models/fr} trained two corpora in French: deep-sequoia and wikiner-fr, capable of named entity recognition.
The global architecture of these pipelines have not been yet published but are explained by the developers on their website.
Words are first encoded into local context-aware embeddings using a window-based CNN similar to~\cite{collobert2011}.
The decision layer is an adaptation of the transition-based model presented in~\cite{lample2016}.
As words are processed sequentially, their vectors are concatenated with those of the last known entities to encode the nearby predicted semantics.
The classification layer relies on a finite-state machine whose transition probabilities are learned using a multi-layer perceptron.
% \bertrand{Drop the next sentences if we need space}
% In 2018~\cite{won2018} evaluated SpaCy's NER ability to detect place names in five corpora of ancient letters written in English.
% They measured an average F1 score of 0.57.
% The SpaCy developers claim an accuracy of 0.85 for the English NER pipeline \textit{en\_core\_web\_lg} on the OntoNotes 5.0 corpus\footnote{https://spacy.io/usage/facts-figures}.
%For our experiments, the French model \textit{fr\_core\_news\_lg} is fine-tuned using our ground truth corpus.



\textbf{CamemBERT} is an adaptation of the BERT transformer-based model for the French language\cite{vaswani2017attention,devlin2018bert}. Such language models have become a new paradigm for NER\cite{li2020}. 
The learned embeddings can be used as distributed representations for input instead of traditional embeddings like Google Word2Vec, and they can be further fine-tuned for NER by adding an additional output layer. 
They can also be pre-trained in an unsupervised way on historical texts for domain adaptation.
CamemBERT \cite{martin-etal-2020-camembert}, is a well-known French language model derived from BERT, of which there are already trained versions for the NER task on the Huggingface library.

