% !TeX root = ../main-paper.tex
\section{OCR and NER on Historical Texts}

The directory processing pipeline presented in \cite{bell2020automated} to locate old gas stations using mid-20th century directories includes an OCR step, done with Tesseract, and a NER step to identify company names and addresses within the extracted text, performed using rules built as regular expressions. This section reviews existing approaches to achieving each of these two tasks. It concludes with a review of the works assessing the effects of OCR quality on the NER performance and the proposed solutions to overcome this problem. 

\subsection{Optical Character Recognition on Historical Texts}

\subsection{Named Entity Recognition}

Many approaches have been designed to recognize named entities, ranging from handcrafted rules to supervised approaches \cite{nadeau2007}. Rule based approaches look for portions of text that match patterns based on word characteristics such as their case, whether they are digits or not, their part-of-speech tags, etc. These patterns can be fully handcrafted like in \cite{bell2020automated} or generated by data mining approaches like in \cite{nouvel2011}. Rule based approaches can also combine a set of patterns with a set of dictionaries (gazetteers, author lists, etc.) that help recognizing named entities when an exhaustive lexicon of searched entities is available \cite{mansouri2008,maurel2011}. For example, the CasEN\footnote{\url{https://tln.lifat.univ-tours.fr/version-francaise/ressources/casen}} transducer cascade leverages syntatic-lexical patterns and dictionnaries to recognize Named Entities in French texts. Such kind of approaches achieve very good results when applied to specialized domain corpus - like directories - and when exhaustive lexicon are available, but at high system engineering cost \cite{nadeau2007}. 

Supervised approaches include both approaches implementing supervised learning algorithms with careful text feature engineering, and deep learning based approaches which automatically build their own features to classify words into one category of named entities or another. In recent years, the latter have grown dramatically, yielding state-of-the art performances and establishing new baselines\cite{li2020}. A recent survey proposed by \cite{li2020} presents deep learning techniques for NER according to the distributed representation(s) they use as input, their context encoder and their tag decoder. It shows that language model embeddings pre-trained using \textit{Transformer} \cite{vaswani2017attention}, like BERT \cite{devlin2018bert}, can not only be used to replace traditional embeddings as input distributed representations but also be fine-tuned for the NER task with one additional output layer while achieving state-of-the-art performances. The survey concludes that fine-tuning general-purpose contextualized language models with domain-specific data is very likely to give good performances for use cases with domain-specific texts and few training data.

\subsection{Named Entity Recognition on Historical Texts}

\cite{Labusch2020NamedED} apply a BERT-based NER model on historical texts in German, French and English, produced by OCR. It is an of-the-shelf multilingual BERT-based NER model, pre-trained on more than 2 000 000 pages of OCRed historical texts, in the three languages and fine-tuned with historical texts NER ground truth, also in the three languages. Eventually, a clear decrease in NER performance on OCRised texts is noted. Moreover, the article reports that this decrease is more important for English texts for which the OCR is of poorer quality than for texts in the other two languages. 

Recently, several studies have focused on the extent to which the quality of the OCR affects the results produced by a NER model.\cite{van2020assessing} assess the impact of OCR on several NLP downstream tasks, including NER. They worked on a corpus published by a post-OCR correction software company, made of many journal articles with different levels of OCR errors and their respective ground-truths. For each OCRed article, the Word Error Rate (WER) is computed and the English model \textit{en-core-web-lg} provided by Spacy\footnote{\url{https://spacy.io/}} library is used to perform NER on \textit{Person}, \textit{GPE} and \textit{Date}. The performance of the NER model with respect to OCR quality is eventually assessed by calculating the F-measure for each NER class, and each article i.e., each WER value. \cite{hamdi2020assessing} performed a similar but more extensive evaluation on four different NER models: CoreNLP using Conditional Random Fields and three deep neural models, BLSTM-CNN, BLSTM-CRF, and BLSTM-CNN-CRF. They tested them on two well-known NER benchmark corpora: CoNLL-02 and CoNLL-03. They applied four different types of OCR noise to each corpus, with two levels of degradation and computed the WER and Character Error Rate (CER) for each degraded version. Finally, they applied each NER model on the progressively degraded versions of the corpora and computed the resulting F-measure. Overall, NER F-mesure drops from 90\% to 50\% when the WER increase from 8\% to 50\%. Besides, all NER models are sensitive to OCR errors, but models based on deep neural networks seem less prone to errors.

Based on the observation that OCR errors necessarily affect the performance of NER approaches applied to historical texts, \cite{huynh2020use}, \cite{martinek2019hybrid} and \cite{marz2021data} have proposed different approaches to reduce their negative impact.





