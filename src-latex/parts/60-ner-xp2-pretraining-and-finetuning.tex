% !TeX root = ../main-paper.tex
\section{Experiment 2: Impact of OCR noise on named entity recognition}
\label{sec:ner-xp2}
Noise introduced by OCR is known to have a negative impact on named entity recognition, because it alters the structure and lexicon of the input texts, moving them away from the language model known to the NER process.
In real-life situations the models used are often trained on texts without such noise, even though the texts to be annotated are extracted with OCR.
This raises the question of the most appropriate strategy to build a NER model tolerant to OCR noise.
In this second experiment we address this question based on our benchmark.


\subsection{Training and evaluation protocol}
Only CmBERT and CmBERT+ptrn are considered since experiment 1 shows that SpaCy NER is systematically outperformed by these two models.

We leverage the labeled datasets NER-reference, NER-pero and NER-tesseract created as explained in \cref{sec:dataset}.
Kraken is left aside as it produces poor results that would require removing 500 entries from all datasets to keep the same set of valid entries.

Each dataset is split into a training, a development and a test set following the exact same protocol as described in \cref{sec:ner-xp1-protocol}, except this time we do not need to create smaller training sets.
The train/dev/test sets are the same size for each NER set as they contain the same entries: 

CmBERT and CmBERT+ptrn are fine-tuneon reference-gold and pero-gold to create two versions of each model: the former trained on manually corrected entries and the latter on OCR entries.
SpaCy NER is left aside as results from experiment 1 show that it is outperformed by BERT models.
Performance metrics are computed for each of the four resulting NER models against the test sets created from reference-gold, pero-gold and tesseract-gold.



Finally each is is split into a training, a dev and a test subset following the procedure presented in\cref{subsection:experiment-1-setup}.



\subsubsection{Huggingface CamemBERT}
\joseph{Copie de \cref{sub:ner-xp1-sysytems-huggingcam}. Je suggère de rédiger la section de l'XP 1 en premier et apporter précision nécessaires ici.}

Experiments 1 and 2 rely on the implementation of transformers provided by the software library Huggingface (transformers v.4.15.0, datasets v.1.17.0).
Our baseline CmBERT model is CamemBERT model published on the Hugging Face repository \footnote{\url{https://huggingface.co/Jean-Baptiste/camembert-ner}} and already trained for NER on wikiner-fr.
Its NER head is a linear model with a Softmax function.
CmBERT and CmBERT+ptrn are always fine-tuned using the same parameters, with at most 5000 training steps and an early stopping condition set to 3 evaluations in experiment 1 (5 in experiment 2) without improvement of the f1 score. Evaluations are performed every 100 steps.

\subsection{Protocol}

We fine-tune CmBERT and CmBERT+ptrn on reference-gold and pero-gold to create two versions of each model: the former trained on manually corrected entries and the latter on OCR entries.
SpaCy NER is left aside as results from experiment 1 show that it is outperformed by BERT models.
Performance metrics are computed for each of the four resulting NER models against the test sets created from reference-gold, pero-gold and tesseract-gold.


\subsection{Results and discussion}


CmBERT and CmBERT+ptrn are fine-tuned on the training sets from reference-gold (manually corrected entries) and pero-gold to produce 4 different models.
Their F1 score measured against the test tests from reference-gold, pero-gold and tesseract-gold are given in \cref{fig:exp_2_eval_ner}.
The results clearly show that models perform best when both the pre-training and the NER fine-tuning share the same characteristics (here, OCR noise) as the texts to be processed.

In our tests, pre-training the model brings a slight gain in performance ($\approx 0.5\%$).
We did not pre-train or fine-tune with texts extracted with Tesseract.
However, despite a loss of performance the model pre-trained and fine-tuned with pero-gold still gives the best results.
This is probably due to the fact that the texts produced by Pero-OCR feature characteristics intermediary between human transcriptions and Tesseract.

\begin{figure}
    \centering
    \includegraphics[width=0.75\textwidth]{images/experiment_2_f1_with_noise_graph.pdf}
    \caption{F1 scores (in \%) of NER predictions in presence of OCR noise in the training and testing examples, either manually corrected or raw from Pero-OCR and Tesseract. The type of examples used to train the NER task is noted in indice after the model name (e.g. Bert$_{reference}$). Results show that best performances on OCRed entries are obtained when the BERT model has been pretrained and fine-tuned for NER on examples affected with similar OCR errors.}
    \label{fig:exp_2_eval_ner}
\end{figure}

