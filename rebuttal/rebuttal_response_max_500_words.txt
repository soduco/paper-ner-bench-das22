We thank the reviewers for their comments. We summarize and respond to them in what follows.


## Metrics and evaluation protocol

> [Reviewer 1] ICDAR Contest in 2011 related to that topic

Thanks for pointing this out. They are essentially using a full-cross evaluation of all methods to perform end-to-end
evaluation, just like us. Also, they do not report NER evaluation, unlike us. We updated the paper with these
clarifications.


> [Reviewer 1] Qualitative analysis of influence of OCR quality on NER

Effects of the OCR noise are indeed non-uniform on NER results. The impact on structuring elements of the entries like
punctuation symbols appears to be most significant, as they mark the separation between semantically homogeneous blocks
(e.g. between PERS and ACT). We plan to carry out a more in-depth qualitative study in the near future.


> [Reviewer 1] The authors take great care to have a clean and reproducible way of projecting expected outcomes onto
> noisy OCR results. The question arises to how useful this is and whether it is not introducing biases in various ways,
> overestimating or underestimating the effects of the NER labeling

The metrics used are the ones commonly used in previous works. Projecting the ground-truth labels on a noisy text was a
sensible way to assess whether NERs could help to correct OCR errors. Here OCR is either pretty good or very wrong, so
alignments usually make sense or can be detected as degenerated cases. However, it is very true that some borderline
cases may cause trouble. This requires more study, and we updated the paper to be more explicit about these limitations.


> [Reviewer 2] Comparing two runs of PERO OCR together, or two runs of Tesseract together

Very good point, but we could not compete with the large, private training datasets used to produce the models we used.
We chose to report, as objectively as possible, the performance of actual, off-the-shelf systems.

> [Reviewer 2] Reason for not reporting Kraken's performance on downstream NER

We chose not to report Kraken's performance as it would have drastically reduced the size of the labelled sets "NER-x" and
thus hampered the other evaluations on the reference, PERO and tesseract datasets. All labelled datasets contain the
same subset of directories entries by construction (section 3). Kraken performs quite poorly, which hinders the
projection of labels from the ground truth and drastically reduces the size of the subset of labelled entries.


> [Reviewer 2] Experiment with artificial OCR noise in future work

We plan to perform such experiments following e.g. the techniques presented in https://doi.org/10.1007/978-3-030-86331-9_48
As it seems to be quite hard to generate realistic noise, we decided to first focus on real cases with no further assumptions.

## Paper organization

> [Reviewer 3] Section 2.3 should be a separate section about the pipeline

We have tried to extract this content as a new section as suggested. However, this change made the sections unbalanced,
and the explication flow disrupted. Moreover, it breaks the 15-pages limits. As a consequence, the overall organization
of the paper was left unchanged.



## Typos, inconsistencies and lack of clarity

> [Reviewer 1] Figure 1 is never referred to in the text
> [Reviewer 1] When referring to figures, you sometimes write "fig. n", "figure n", and "Figure n"
> [Reviewer 1] You wrote Tesseract twice with a lowercase t

Thanks for pointing these issues, we fixed them.

> [Reviewer 3] What are "thumbnails" in page 7

We refer to "thumbnails" as image crops, they have the same resolution as the original image. We updated the text to
clarify this.

> [Reviewer 3] Description of NER QA is confusing
> [Reviewer 3] Figure 4 needs clarification

We clarified the relevant section and fig.4's caption.
