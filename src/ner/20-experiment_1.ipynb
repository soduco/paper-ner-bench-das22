{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2552858d-7386-4e9a-8b0e-c338b920f783",
   "metadata": {},
   "source": [
    "# 20 - Experiment #1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ddcc84-3134-41bb-8c3c-f9e902a8cfb9",
   "metadata": {},
   "source": [
    "**Requirements** \n",
    "\n",
    "Do not forget to install the SpaCy pipeline `fr_core_news_lg` before running Experiment 1:\n",
    "```bash\n",
    "python -m spacy download fr_core_news_lg\n",
    "```\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06332f9f-37d5-4a0c-9af6-1f2bda236789",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# GLOBAL CONSTANTS\n",
    "N_RUNS = 1 # Train only once per trainset size. Change this value to run multiple trainings for each model x trainset\n",
    "\n",
    "# Expected datasets indexed by number of examples in the trainset\n",
    "TRAINSETS_SIZES = [49,99,199,398,769,1593,3186,6373]\n",
    "TRAINSETS_SIZES = [199]\n",
    "\n",
    "# INPUT / OUTPUT DIRS\n",
    "nb_loc = Path(os.path.dirname(os.path.realpath(\"__file__\"))).resolve()\n",
    "INPUT_DIR = nb_loc / \"00-prepared_datasets/01-experiment_1\"\n",
    "METRICS_OUTPUT_DIR = nb_loc / \"20-experiment_1_metrics\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3113af67-7896-4a87-a904-26130e5cb47d",
   "metadata": {},
   "source": [
    "## 21. SpaCy NER pipeline - train & eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34cc9601-6e2b-4e71-90b7-6889104dcfb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPACY CONSTS\n",
    "SPACY_NER_METRICS_DIR = METRICS_OUTPUT_DIR / \"21-spacy_ner\"\n",
    "SPACY_NER_METRICS_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# Spacy's model be overwritten by each run & for each trainset size.\n",
    "# The last model will be trained on the largest trainset.\n",
    "SPACY_SAVE_MODEL_PATH = \"/tmp\"\n",
    "SPACY_USE_GPU = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c86974-f989-406c-b4da-1e881457e333",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16/01/2022 02:40:29 ; INFO ; SpaCy run #1, will save in /home/bertrand/dev/paper-ner-bench-das22/src/ner/20-experiment_1_metrics/21-spacy_ner/run_1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Saving to output directory: /tmp\u001b[0m\n",
      "\u001b[38;5;4mℹ Using CPU\u001b[0m\n",
      "\u001b[1m\n",
      "=========================== Initializing pipeline ===========================\u001b[0m\n",
      "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
      "\u001b[1m\n",
      "============================= Training pipeline =============================\u001b[0m\n",
      "\u001b[38;5;4mℹ Pipeline: ['tok2vec', 'ner']\u001b[0m\n",
      "\u001b[38;5;4mℹ Initial learn rate: 0.001\u001b[0m\n",
      "E    #       LOSS TOK2VEC  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
      "---  ------  ------------  --------  ------  ------  ------  ------\n",
      "  0       0          0.00     55.74   23.37   30.55   18.93    0.23\n",
      "  8     200          0.00   2289.49   87.86   87.69   88.04    0.88\n",
      " 18     400          0.00    177.48   88.13   87.86   88.39    0.88\n",
      " 31     600          0.00     12.94   87.78   87.42   88.14    0.88\n",
      " 47     800          0.00      4.55   87.81   87.40   88.21    0.88\n",
      " 66    1000          0.00     13.56   88.15   87.81   88.50    0.88\n",
      " 90    1200          0.00      2.45   87.86   87.55   88.18    0.88\n",
      "118    1400          0.00      4.52   87.62   87.17   88.07    0.88\n",
      "153    1600          0.00     25.64   87.28   87.17   87.39    0.87\n"
     ]
    }
   ],
   "source": [
    "from spacy.cli import train, evaluate\n",
    "from config import logger\n",
    "\n",
    "# Train & evaluate loop\n",
    "for run in range(1, N_RUNS + 1):\n",
    "    output_dir = SPACY_NER_METRICS_DIR / f\"run_{run}\"\n",
    "    output_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    logger.info(f\"SpaCy run #{run}, will save in {output_dir}\")\n",
    "    \n",
    "    for trainset_size in TRAINSETS_SIZES:\n",
    "        # paths to datasets\n",
    "        trainset = INPUT_DIR / f\"spacy_train_{trainset_size}.spacy\"\n",
    "        devset = INPUT_DIR / f\"spacy_dev_{trainset_size}.spacy\"\n",
    "        testset = INPUT_DIR / f\"spacy_test_{trainset_size}.spacy\"\n",
    "\n",
    "        # Pass train & dev paths as SpaCy config items\n",
    "        spacy_opts = {\n",
    "            \"paths.train\": str(trainset),\n",
    "            \"paths.dev\": str(devset),\n",
    "        }\n",
    "        \n",
    "        # Train now !\n",
    "        train.train(\"cnn_config.cfg\",       # The pipeline configuration file\n",
    "                    SPACY_SAVE_MODEL_PATH,  # save model-best and model-last here\n",
    "                    use_gpu=SPACY_USE_GPU,  # Use GPU if asked\n",
    "                    overrides=spacy_opts)   # Pass training options\n",
    "\n",
    "        model_best = SPACY_SAVE_MODEL_PATH / \"model-best\"\n",
    "        \n",
    "        # Compute metrics on the test set\n",
    "        metrics_file = output_dir / f\"test_{trainset_size}.json\"\n",
    "        evaluate(model_best,                      # Where is the trained model\n",
    "                 testset,                         # Test dataset\n",
    "                 metrics_file,                    # Save metrics here\n",
    "                 use_gpu=SPACY_USE_GPU,           # Use GPU if asked\n",
    "                 displacy_path=output_dir,        # Save a few tagged results to be shown with displacy\n",
    "                 displacy_limit=100)              # How much is \"a few\"\n",
    "        \n",
    "        # Compute metrics on the dev set\n",
    "        metrics_file = output_dir / f\"dev_{trainset_size}.json\"\n",
    "        evaluate(model_best,                      # Where is the trained model\n",
    "                 devset,                          # Dev dataset\n",
    "                 metrics_file,                    # Save metrics here\n",
    "                 use_gpu=SPACY_USE_GPU,           # Use GPU if asked\n",
    "                 displacy_path=output_dir,        # Save a few tagged results to be shown with displacy\n",
    "                 displacy_limit=100)              # How much is \"a few\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b18a5bc-1abb-450d-90d3-6a7e56f773ed",
   "metadata": {},
   "source": [
    "## 22. CamemBERT - Common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91140aa5-b377-47c1-bd44-844cd9365ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMMON CONSTANTS\n",
    "\n",
    "TRAINING_CONFIG = {\n",
    "    \"evaluation_strategy\": \"steps\",\n",
    "    \"eval_steps\": 100,\n",
    "    \"max_steps\": 5000,\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"per_device_train_batch_size\": 16,\n",
    "    \"per_device_eval_batch_size\": 16,\n",
    "    \"weight_decay\": 1e-5,\n",
    "    \"load_best_model_at_end\": True,\n",
    "    \"greater_is_better\":True,\n",
    "    \"metric_for_best_model\": \"f1\",\n",
    "    \"save_strategy\": \"steps\",\n",
    "    \"save_steps\": 100, # Make Early callback bug ?\n",
    "    \"save_total_limit\": 1,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e821087-3623-4c14-a8fb-63dcc98dc1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import logger\n",
    "from datasets import load_from_disk\n",
    "from camembert_util import train_eval_loop\n",
    "\n",
    "def train_bert(metrics_output_directory):\n",
    "    # Train & evaluate loop\n",
    "    for run in range(1, N_RUNS + 1):\n",
    "        output_dir = metrics_output_directory / f\"run_{run}\"\n",
    "        output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "        logger.info(f\"{model} #{run}, will save in {output_dir}\")\n",
    "\n",
    "        for trainset_size in TRAINSETS_SIZES:\n",
    "            datasetdir = WORK_DIR / f\"huggingface_{trainset_size}\"\n",
    "\n",
    "            logger.info(f\"Running on datasets in {datasetdir}\")\n",
    "            logger.info(f\"Metrics will be saved in {output_dir}\")\n",
    "\n",
    "            # Train now !\n",
    "            train_dev_test = load_from_disk(datasetdir)\n",
    "            metrics = train_eval_loop(model,         # Implicit. Must be setbefore calling train_bert()\n",
    "                                      training_args, # Idem\n",
    "                                      tokenizer,     # idem\n",
    "                                      **train_dev_test)\n",
    "\n",
    "            # Save the metrics\n",
    "            metrics_file = output_dir / f\"test_{trainset_size}.json\"\n",
    "            with open(metrics_file, \"w\", encoding=\"utf-8\") as o:\n",
    "                json.dump(metrics[0], o)\n",
    "\n",
    "            metrics_file = output_dir / f\"dev_{trainset_size}.json\"\n",
    "            with open(metrics_file, \"w\", encoding=\"utf-8\") as o:\n",
    "                json.dump(metrics[1], o)\n",
    "\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09de0446-e62f-46d5-ae73-34112f3c420d",
   "metadata": {},
   "source": [
    "## 23 - CamemBERT - train & eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c808f0e4-1b8a-43d7-9b16-256e60faf339",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CAMEMBERT CONSTS\n",
    "CAMEMBERT_METRICS_DIR = METRICS_OUTPUT_DIR / \"22-camembert\"\n",
    "CAMEMBERT_METRICS_DIR.mkdir(exist_ok=True, parents=True)\n",
    "CAMEMBERT_MODEL = \"Jean-Baptiste/camembert-ner\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05387e39-dd69-491e-9517-57490356e5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from camembert_util import init_model\n",
    "\n",
    "# Get the model components\n",
    "model, tokenizer, training_args = init_model(CAMEMBERT_MODEL, TRAINING_CONFIG)\n",
    "\n",
    "# Run the main loop\n",
    "train_bert(CAMEMBERT_METRICS_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17cd632b-99a9-4c85-aafc-72b7b0615d60",
   "metadata": {},
   "source": [
    "## 23 - CamemBERT pretrained - train & eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0b70e9-fc3e-47da-b231-002d329a2932",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CAMEMBERT PRETRAINED CONSTS\n",
    "CAMEMBERT_PRETRAINED_METRICS_DIR = METRICS_OUTPUT_DIR / \"23-camembert_pretrained\"\n",
    "CAMEMBERT_PRETRAINED_METRICS_DIR.mkdir(exist_ok=True, parents=True)\n",
    "CAMEMBERT_PRETRAINED_MODEL = \"HueyNemud/berties-pretrained-das22\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3459be-8661-40f4-a9dc-e4bc03f9cb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from camembert import init_model\n",
    "\n",
    "# Get the model components\n",
    "model, tokenizer, training_args = init_model(CAMEMBERT_PRETRAINED_MODEL, TRAINING_CONFIG)\n",
    "\n",
    "# Run the main loop\n",
    "train_bert(CAMEMBERT_PRETRAINED_METRICS_DIR)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
